from __future__ import annotations
import argparse, warnings, time, os
from pathlib import Path
from typing import Optional, Iterable, Tuple, List
from PIL import Image
import numpy as np
from sklearn.metrics import confusion_matrix

try:
    from skimage.feature import hog, canny
    from skimage import filters, exposure, color
    _HAVE_SKIMAGE = True
except Exception:
    _HAVE_SKIMAGE = False

try:
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support
    from sklearn.preprocessing import LabelEncoder, StandardScaler
    from sklearn.pipeline import make_pipeline
    import joblib
    _HAVE_SKLEARN = True
except Exception:
    _HAVE_SKLEARN = False

DEFAULT_MODEL_PATH = Path(__file__).resolve().parent / "trained_model.joblib"

def list_image_files(dataset_dir: Path, exts: Tuple[str, ...] = (".jpg", ".jpeg", ".png", ".bmp")):
    d = Path(dataset_dir)
    if not d.exists(): return {}
    out = {}
    for child in sorted(d.iterdir()):
        if child.is_dir():
            files = [p for p in child.rglob("*") if p.suffix.lower() in exts]
            if files: out[child.name] = files
    return out

def extract_features(img: Image.Image, use_hog: bool = True, preprocess: bool = True, threshold_segmentation: bool = True):
    img = img.convert("RGB")
    arr = np.array(img)
    if preprocess and _HAVE_SKIMAGE:
        gray = color.rgb2gray(arr)
        gray_smooth = filters.gaussian(gray, sigma=1.0, preserve_range=True)
        edges = canny(gray_smooth, sigma=1.0)
        if threshold_segmentation:
            thresh = filters.threshold_otsu(gray_smooth)
            mask = gray_smooth > thresh
            gray_proc = gray_smooth * mask
        else:
            gray_proc = gray_smooth
        arr_proc = np.stack([gray_proc, edges.astype(float), gray_smooth], axis=-1)
        arr_proc = exposure.rescale_intensity(arr_proc, out_range=(0, 255)).astype(np.uint8)
    else:
        arr_proc = arr

    bins = 64
    feats = []
    for c in range(arr_proc.shape[2]):
        h, _ = np.histogram(arr_proc[:, :, c], bins=bins, range=(0, 255), density=True)
        feats.append(h)
    feats = np.concatenate(feats)

    if use_hog and _HAVE_SKIMAGE:
        try:
            gray_hog = color.rgb2gray(arr_proc)
            hog_vec = hog(gray_hog, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(3, 3),
                          block_norm="L2-Hys", transform_sqrt=True, feature_vector=True)
            feats = np.concatenate([feats, hog_vec])
        except Exception:
            pass

    return feats.astype("float32")

def build_dataset(dataset_dir: Path, max_per_class: Optional[int] = None):
    files_by_label = list_image_files(dataset_dir)
    if not files_by_label:
        return np.zeros((0, 0), dtype="float32"), np.array([]), [], []
    X_list, y_list, fnames = [], [], []
    for label in sorted(files_by_label.keys()):
        files = files_by_label[label]
        if max_per_class: files = files[:max_per_class]
        for p in files:
            try:
                img = Image.open(p).convert("RGB").resize((128, 128))
                feats = extract_features(img, use_hog=True, preprocess=True)
                X_list.append(feats); y_list.append(label); fnames.append(str(p))
            except Exception as e:
                warnings.warn(f"Skipping {p}: {e}")
    X = np.stack(X_list, axis=0)
    y = np.array(y_list)
    labels = sorted(list(set(y_list)))
    return X, y, labels, fnames

def train(dataset_dir: Path, model_path: Path = DEFAULT_MODEL_PATH, test_size: float = 0.2, random_state: int = 42, max_per_class: Optional[int] = None):
    if not _HAVE_SKLEARN: raise RuntimeError("scikit-learn and joblib are required.")
    print(f"üìÇ Loading dataset from: {dataset_dir}")
    X, y, labels, filenames = build_dataset(dataset_dir, max_per_class=max_per_class)
    if X.size == 0: raise RuntimeError(f"No images found in {dataset_dir}.")
    le = LabelEncoder(); y_enc = le.fit_transform(y); classes = le.classes_
    print(f"üß© Loaded {len(X)} samples across {len(classes)} classes.")
    X_train, X_test, y_train, y_test, fnames_train, fnames_test = train_test_split(X, y_enc, filenames, test_size=test_size, stratify=y_enc, random_state=random_state)
    clf = RandomForestClassifier(n_estimators=800, max_depth=None, min_samples_split=2, min_samples_leaf=1,
                                 max_features="sqrt", bootstrap=True, oob_score=True, class_weight="balanced_subsample",
                                 n_jobs=-1, random_state=random_state)
    model = make_pipeline(StandardScaler(with_mean=False), clf)
    print("üöÄ Training model...")
    t0 = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - t0
    print(f"‚úÖ Training done in {train_time:.2f}s")
    print("üìä Evaluating...")
    y_pred = model.predict(X_test)
    try:
        y_score = model.predict_proba(X_test)
    except Exception:
        y_score = None
    acc = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    cm_norm = confusion_matrix(y_test, y_pred, normalize="true")
    prec, rec, f1, support = precision_recall_fscore_support(y_test, y_pred, labels=range(len(classes)))
    per_class_metrics = {cls: {"precision": float(prec[i]), "recall": float(rec[i]), "f1": float(f1[i]), "support": int(support[i])} for i, cls in enumerate(classes)}
    print(f"‚úÖ Accuracy: {acc:.4f}")
    for cls, vals in per_class_metrics.items():
        print(f"  {cls:10s} | Prec: {vals['precision']:.3f} | Rec: {vals['recall']:.3f} | F1: {vals['f1']:.3f}")
    payload = {"model": model, "classes": list(classes), "accuracy": float(acc), "train_time_sec": float(train_time),
               "X_train": X_train, "y_train": y_train, "X_test": X_test, "y_test": y_test, "y_pred": y_pred, "y_score": y_score,
               "confusion_matrix": cm, "confusion_matrix_norm": cm_norm, "per_class_metrics": per_class_metrics,
               "filenames_test": [os.path.basename(f) for f in fnames_test], "created_at": time.strftime("%Y-%m-%d %H:%M:%S")}
    joblib.dump(payload, model_path)
    print(f"üíæ Saved model payload to {model_path}")
    return {"model_path": str(model_path), "accuracy": float(acc), "report": classification_report(y_test, y_pred, target_names=classes)}

class TrashModel:
    def __init__(self, model_path: Optional[str] = None):
        self.model_path = Path(model_path) if model_path else DEFAULT_MODEL_PATH
        self._loaded = False
        self._load_model()

    def _load_model(self):
        if not _HAVE_SKLEARN or not self.model_path.exists():
            self._loaded = False; self._fallback_labels = ["garbage", "compost", "recycle"]; return
        import joblib
        payload = joblib.load(self.model_path)
        self.model = payload.get("model"); self.classes = payload.get("classes", []); self._loaded = True

    def predict_topk(self, img: Image.Image, k: int = 5) -> List[Tuple[str, float]]:
        try:
            img_proc = img.convert("RGB").resize((128, 128)); feats = extract_features(img_proc)
        except Exception:
            return []
        if self._loaded:
            try:
                probs = self.model.predict_proba([feats])[0]; pairs = list(zip(self.classes, probs)); pairs.sort(key=lambda x: x[1], reverse=True); return pairs[:k]
            except Exception:
                try:
                    label, conf = self.predict(img); return [(label, conf)]
                except Exception:
                    return [(self._fallback_labels[i % len(self._fallback_labels)], 0.6 - 0.1 * i) for i in range(k)]
        else:
            return [(self._fallback_labels[i % len(self._fallback_labels)], float(max(0.0, 0.6 - 0.1 * i))) for i in range(k)]

    def predict(self, img: Image.Image):
        try:
            img_proc = img.convert("RGB").resize((128, 128)); feats = extract_features(img_proc)
        except Exception:
            return (self._fallback_labels[0], 0.0)
        if self._loaded:
            try:
                probs = self.model.predict_proba([feats])[0]; idx = int(np.argmax(probs)); return (self.classes[idx], float(probs[idx]))
            except Exception:
                try:
                    pred = self.model.predict([feats])[0]
                    if isinstance(pred, (int, np.integer)): return (self.classes[int(pred)], 0.5)
                    return (str(pred), 0.5)
                except Exception:
                    return (self._fallback_labels[(img.size[0] + img.size[1]) % 3], 0.6)
        else:
            return (self._fallback_labels[(img.size[0] + img.size[1]) % 3], 0.6)

def _cli_train(args: argparse.Namespace):
    model_path = Path(args.model_path) if args.model_path else DEFAULT_MODEL_PATH
    print(f"üìÅ Training on dataset: {args.dataset}")
    try:
        res = train(Path(args.dataset), model_path=model_path, test_size=args.test_size, max_per_class=args.max_per_class)
        print(f"‚úÖ Saved model: {res['model_path']}"); print(f"üìà Accuracy: {res['accuracy']:.4f}"); print(res["report"])
    except Exception as e:
        print("‚ùå Training failed:", e)

def _cli_predict(args: argparse.Namespace):
    tm = TrashModel(args.model_path); img = Image.open(args.image); label, conf = tm.predict(img)
    print(f"Prediction: {label} (confidence: {conf:.3f})")

def main(argv: Optional[Iterable[str]] = None):
    p = argparse.ArgumentParser(description="Train or predict using SmartTrashSorter model")
    sub = p.add_subparsers(dest="cmd")
    t = sub.add_parser("train"); t.add_argument("--dataset", required=True); t.add_argument("--model-path", default=str(DEFAULT_MODEL_PATH)); t.add_argument("--test-size", type=float, default=0.2); t.add_argument("--max-per-class", type=int, default=None)
    pr = sub.add_parser("predict"); pr.add_argument("--image", required=True); pr.add_argument("--model-path", default=str(DEFAULT_MODEL_PATH))
    args = p.parse_args(list(argv) if argv else None)
    if args.cmd == "train": _cli_train(args)
    elif args.cmd == "predict": _cli_predict(args)
    else: p.print_help()

if __name__ == "__main__":
    main()
